<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="../styles/style.css">
    <link rel="stylesheet" href="../styles/article-style.css">
    <script src="../scripts/script.js" defer></script>
    <script src="../scripts/comments-section1.js" defer></script>
</head>
<body>
    <header>
        <img src="../images/logo.png" alt="logo philosophy channel" width="200">
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../articles.html">Articles</a></li>
                <li><a href="#podcasts">Podcasts</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <main class="article-container">
            <article class="full-article">
                <!-- Main Title -->
                <h1 class="article-title">The Ethics of Artificial Intelligence</h1>
                
                <!-- Article Metadata -->
                <div class="article-meta">
                    <span class="author">By Isaac Maldonado</span>
                    <span class="date">Published August 20, 2023</span>
                    <span class="reading-time">8 min read</span>
                </div>
                
                <!-- Featured Image -->
                <figure class="article-featured-image">
                    <img src="../images/IA.jpeg" alt="AI Ethics Concept Art" class="article-image">
                    <figcaption>Human and machine hands reaching toward each other</figcaption>
                </figure>
                
                <!-- Article Content -->
                <div class="article-content">
                    <p class="article-intro">AI's ethical crossroads: From bias in algorithms to existential risks, we examine the urgent moral dilemmas of artificial intelligence. Who decides what's "ethical" when machines think?</p>
                    
                    <h2>The Algorithmic Bias Crisis</h2>
                    <p>Recent studies show 78% of facial recognition systems demonstrate racial bias, while hiring algorithms frequently discriminate against women. This reveals:</p>
                    <ul>
                        <li>Training data reflects human prejudices</li>
                        <li>Lack of diversity in tech teams</li>
                        <li>The myth of technological neutrality</li>
                    </ul>
                    <img src="../images/que-es-un-algoritmo.jpg" alt="Algorithmic Bias Diagram" class="article-inline-image">
                    
                    <h2>Autonomous Weapons: The New Moral Frontier</h2>
                    <p>The development of lethal autonomous weapons systems (LAWS) presents unprecedented ethical challenges:</p>
                    <div class="ethics-grid">
                        <div class="issue">
                            <h4>Accountability Gap</h4>
                            <p>Who's responsible when an AI weapon makes a fatal error? The programmer? The military commander? The algorithm itself?</p>
                        </div>
                        <div class="issue">
                            <h4>Proliferation Risks</h4>
                            <p>Unlike nuclear weapons, AI weapons require no rare materials, making them potentially accessible to non-state actors.</p>
                        </div>
                    </div>
                    
                    <h2>Existential Risks and Superintelligence</h2>
                    <p>Philosophers like Nick Bostrom warn about AI alignment problems that could emerge if we create superintelligent systems:</p>
                    <ol>
                        <li>The control problem (how to constrain smarter-than-human AI)</li>
                        <li>Value alignment (encoding human ethics into machines)</li>
                        <li>Instrumental convergence (AI developing dangerous sub-goals)</li>
                    </ol>
                    <blockquote>"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else." <cite>- Eliezer Yudkowsky</cite></blockquote>
                    
                    <div class="call-to-action">
                        <button onclick="window.location.href='ai-regulation-solutions.html'" class="read-more-btn">Explore Policy Solutions</button>
                    </div>
                </div>
                
                <!-- Comments Section -->
                <section class="comments-section">
                    <h3>Comments (<span id="comments-lenght"></span>)</h3>
                    
                    <div class="comment-form">
                        <h4>Leave a Comment</h4>
                        <form action="#" method="post">
                            <div class="form-group">
                                <label for="name">Name:</label>
                                <input type="text" id="name" name="name" required>
                            </div>
                            <div class="form-group">
                                <label for="email">Email (not published):</label>
                                <input type="email" id="email" name="email" required>
                            </div>
                            <div class="form-group">
                                <label for="comment">Comment:</label>
                                <textarea id="comment" name="comment" rows="5" required></textarea>
                            </div>
                            <button id="submit-comment" class="submit-btn">Post Comment</button>
                        </form>
                    </div>
                    
                    <div class="comment-list">
                        <div class="comment">
                            <div class="comment-header">
                                <span class="comment-author">Dr. Elena Zhang</span>
                                <span class="comment-date">August 22, 2023</span>
                            </div>
                            <div class="comment-body">
                                <p>Missing discussion of China's AI ethics framework which prioritizes collective harmony over individual rights.</p>
                            </div>
                        </div>
                        
                        <div class="comment">
                            <div class="comment-header">
                                <span class="comment-author">Raj Patel</span>
                                <span class="comment-date">August 21, 2023</span>
                            </div>
                            <div class="comment-body">
                                <p>How do we prevent ethical AI from becoming another form of Western cultural imperialism?</p>
                            </div>
                        </div>
                    </div>
                </section>
            </article>
            
            <!-- Sidebar -->
            <aside class="article-sidebar">
                <h3>Related Articles</h3>
                <ul class="related-articles">
                    <li><a href="transhumanism-ethics.html">Transhumanism: The Next Evolution</a></li>
                    <li><a href="quantum-computing-risks.html">Quantum Computing's Ethical Timebomb</a></li>
                    <li><a href="neurotechnology-rights.html">The Coming Neurotechnology Rights Movement</a></li>
                    <li><a href="ai-art-copyright.html">Who Owns AI-Generated Art?</a></li>
                </ul>
    
                <div class="philosopher-quotes">
                    <h3>AI Ethics Quotes</h3>
                    <blockquote>
                        "The question isn't whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions."
                        <cite>- Marvin Minsky</cite>
                    </blockquote>
                    <blockquote>
                        "By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it."
                        <cite>- Stuart Russell</cite>
                    </blockquote>
                </div>
            </aside>
        </main>
    </main>
    <footer>
        <p>Â©<span id="currentYear"></span> Philosophy Channel - Isaac Maldonado - BYU Idaho</p>
        <p>Last Modification: <span id="last-modification"></span></p>
    </footer>
</body>
</html>